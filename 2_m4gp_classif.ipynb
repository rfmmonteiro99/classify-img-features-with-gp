{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e12c129",
   "metadata": {},
   "source": [
    "# Stage 2: Image Feature Analysis\n",
    "\n",
    "**Rui Filipe Martins Monteiro (R20170796) | MSc in Data Science and Advanced Analytics**\n",
    "\n",
    "<br>\n",
    "\n",
    "This notebook loads the intermediate datasets and performs classification with M4GP. Significance tests are performed on the output to determine statistical significance.\n",
    "\n",
    "[ellyn](https://cavalab.org/ellyn/), a genetic programming system for regression, is one of the most important Python libraries, as it is needed to apply M4GP. *Note:* As of February 2022, ellyn is not compatible with Windows, so this needs to be run on Linux.\n",
    "\n",
    "<br>\n",
    "\n",
    "This code is heavily inspired by: Jonathan Janke (https://github.com/novajon/classy-conv-features)\n",
    "\n",
    "Code changed and improved by: Rui Monteiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f8eb222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "from ellyn import ellyn\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    confusion_matrix, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    roc_auc_score, \n",
    "    cohen_kappa_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, KFold\n",
    "\n",
    "# import keras\n",
    "# from keras.utils import to_categorical\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import pickle\n",
    "import statistics\n",
    "# from scipy import stats\n",
    "\n",
    "# from scipy.stats import ttest_ind_from_stats\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e56a084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_over_datasets(datasets, function):\n",
    "    \"\"\"Iterate over datasets and apply function on each one\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datasets : list\n",
    "        list of datasets that function should be applied on\n",
    "    function : function\n",
    "        function that should be applied on each dataset\n",
    "    \"\"\"\n",
    "    ret_data = []\n",
    "    \n",
    "    for data in datasets:\n",
    "        n_data = function(data)\n",
    "        ret_data.append(n_data)\n",
    "        \n",
    "    return ret_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f56678db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target (data):\n",
    "    \"\"\"Save the target data separately from the input data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : list\n",
    "        list containing input and output data together\n",
    "    \"\"\"\n",
    "    target = []\n",
    "    inp = []\n",
    "    \n",
    "    for d in data:\n",
    "        target.append(d[1][0])\n",
    "        inp.append(d[0])\n",
    "        \n",
    "    return [np.array(inp), target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a69716a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex_classes(data):\n",
    "    \"\"\"Reindexing classes to be continuous and start at 0\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : list\n",
    "        dataset to reindex\n",
    "    \"\"\"\n",
    "    for ind, ds in enumerate(data):\n",
    "        ds[1] = [x - min (ds[1]) for x in ds[1]]\n",
    "        data[ind] = ds\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95c54ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_accuracy(input_target, input_data, n=5):\n",
    "    \"\"\"Get top n accuracy from input data (predictions)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_target : list\n",
    "        array of correct targets\n",
    "    input_data : list\n",
    "        array of predictions\n",
    "    n : int\n",
    "        n value for top-n-accuracy\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    inp_data = input_data.copy()\n",
    "    \n",
    "    for ind, pred in enumerate(inp_data):\n",
    "        max_classes = []\n",
    "        \n",
    "        for i in range(n):\n",
    "            max_classes.append(np.argmax(pred))\n",
    "            pred[np.argmax(pred)]=-1\n",
    "            \n",
    "        if input_target[ind] in max_classes: \n",
    "            count += 1\n",
    "            \n",
    "    return count/len(input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58433483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    \"\"\"Method to evaluate a model based on several metrics:\n",
    "        - top-1-accuracy (\"accuracy\")\n",
    "        - top-2-accuracy\n",
    "        - top-5-accuracy\n",
    "        - top-10-accuracy\n",
    "        - top-20-accuracy\n",
    "        - confusion matrix\n",
    "        - precision\n",
    "        - recall\n",
    "        - f1-score\n",
    "        - cohen's kappa\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : keras/sklearn model (needs predict function)\n",
    "        dataset to reindex\n",
    "\n",
    "    data : list\n",
    "        dataset to evaluate\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(data[0])\n",
    "    \n",
    "    try: y_proba = model.predict_proba(data[0])\n",
    "    except: y_proba = model.predict(data[0])\n",
    "    \n",
    "    try: predictions = [round(value) for value in y_pred]\n",
    "    except: predictions = [np.argmax(value) for value in y_pred]\n",
    "    \n",
    "    # Evaluate predictions\n",
    "    d = {}\n",
    "    d[\"accuracy\"] = accuracy_score(data[1], predictions)\n",
    "    print('y_proba, is it a problem?:', y_proba) # DEL LATER!\n",
    "    #d[\"top-2-accuracy\"] = get_top_n_accuracy(data[1], y_proba, n=2)\n",
    "    #d[\"top-5-accuracy\"] = get_top_n_accuracy(data[1], y_proba, n=5)\n",
    "    #d[\"top-10-accuracy\"] = get_top_n_accuracy(data[1], y_proba, n=10)\n",
    "    #d[\"top-20-accuracy\"] = get_top_n_accuracy(data[1], y_proba, n=20)\n",
    "    d[\"confusion matrix\"] = confusion_matrix(data[1], predictions)\n",
    "    d[\"precision\"] = precision_score(data[1], predictions, average='macro')\n",
    "    d[\"recall\"] = recall_score(data[1], predictions, average='macro')\n",
    "    d[\"f1-score\"] = f1_score(data[1], predictions, average='macro')\n",
    "    d[\"cohen's kappa\"] = cohen_kappa_score(data[1], predictions)\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5ba3bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_measures(measure, data_type=\"data\", measure_name=\"Accuracy\"):\n",
    "    \"\"\"Get string representation of measures\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    measure : float\n",
    "        value to print\n",
    "    data_type : string\n",
    "        usually test or training data\n",
    "    measure_name : string\n",
    "        measure that is applied, e.g. 'Accuracy'\n",
    "    \"\"\"\n",
    "    return \"%s in %s: %.2f\" % (measure_name, data_type, measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0e9abfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_measures(evaluation, t):\n",
    "    \"\"\"Print measures on screen\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    evaluation : dict\n",
    "        key value pairs with accuracy measure : value\n",
    "    t : string\n",
    "        Data type, e.g. \"Test\" or \"Train\"\n",
    "    \"\"\"\n",
    "    for key in evaluation.keys():\n",
    "        if key != \"confusion matrix\":\n",
    "            print(get_measures(evaluation[key], t, key))\n",
    "            \n",
    "        else:\n",
    "            print(key)\n",
    "            print(evaluation[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72168829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(train_eval, test_eval, model_stats, model_key, model_name, output_file_name):\n",
    "    \"\"\"Save the results to a CSV file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_eval : dict\n",
    "        key value pairs of \"evaluation metric name : evaluation metric value\" on train data\n",
    "    test_eval : dict\n",
    "        key value pairs of \"evaluation metric name : evaluation metric value\" on test data\n",
    "    model_stats : dict\n",
    "        key value pair of \"train\"/\"test\" : model statistics (e.g., mean, std. dev, ...)\n",
    "    model_key : string\n",
    "        name of the model that was applied\n",
    "    model_name : string\n",
    "        name of the model that was used to create the intermediate dataset, e.g., VGG-16\n",
    "    output_file_name : string\n",
    "        name of the file to write to\n",
    "    \"\"\"\n",
    "    file_stamp = curr_path.split(\"/\")[-2]\n",
    "    train_size = str(len(train_data[0][0]))\n",
    "    test_size = str(len(test_data[0][0]))\n",
    "    dataset_dim = str(np.max(train_data[0][1] + test_data[0][1]) + 1)\n",
    "    \n",
    "    with open(output_file_name + '_train.csv', 'a') as f:\n",
    "        print (output_file_name + '_train.csv')\n",
    "        f.write(\"\\n\" + model_name + ',' + file_stamp + ',' + train_size + ',' + dataset_dim + ',' + model_key + \",\")\n",
    "        f.write(','.join([str(train_eval[x]) for x in train_eval.keys() if x!=\"confusion matrix\"]) + \",\")\n",
    "        f.write(','.join([str(x) for x in model_stats[\"train\"].values()]))\n",
    "    \n",
    "    with open(output_file_name + '_test.csv', 'a') as f:\n",
    "        f.write(\"\\n\" + model_name + ',' + file_stamp + ',' + test_size + ',' + dataset_dim + ',' + model_key + \",\")\n",
    "        f.write(','.join([str(test_eval[x]) for x in test_eval.keys() if x!=\"confusion matrix\"]) + \",\")    \n",
    "        f.write(','.join([str(x) for x in model_stats[\"test\"].values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9750e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_files(ind,prefix):\n",
    "    \"\"\"Create initial files to write to\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ind : int\n",
    "        index of dataset\n",
    "    prefix : string\n",
    "        prefix to give to file name\n",
    "    \"\"\"\n",
    "    path = \"/home/ruifi/thesis/outputs/benchmark_results/\" + prefix + \"_\" + dataset_name + \"/\"\n",
    "    output_file_name = path + prefix + \"_\" + str(ind)\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    with open(output_file_name + '_train.csv', 'w') as f:\n",
    "        f.write('Dataset,Dataset Stamp,Dataset Size,# Dataset Classes,Model Architecture' \n",
    "                + ',' + \"accuracy\" + ',' + \"top-2-accuracy\" + ',' + \"top-5-accuracy\" + ',' \n",
    "                + \"top-10-accuracy\" + ',' + \"top-20-accuracy\" + ',' + \"precision\" + ',' \n",
    "                + \"recall\" + ',' + \"f1-score\" + ',' + \"cohen's kappa\" + ',' \n",
    "                + \"Mean Accuracy over cross validation\" + ',' + \"mean standard deviation over cross validation\" \n",
    "                + ',' + \"number observations over cross validation\")\n",
    "        \n",
    "    with open(output_file_name + '_test.csv', 'w') as f:\n",
    "        f.write('Dataset,Dataset Stamp,Dataset Size,# Dataset Classes,Model Architecture' \n",
    "                + ',' + \"accuracy\" + ',' + \"top-2-accuracy\" + ',' + \"top-5-accuracy\" + ',' \n",
    "                + \"top-10-accuracy\" + ',' + \"top-20-accuracy\" + ',' + \"precision\" \n",
    "                + ',' + \"recall\" + ',' + \"f1-score\" + ',' + \"cohen's kappa\" + ',' \n",
    "                + \"Mean Accuracy over cross validation\" + ',' + \"mean standard deviation over cross validation\" \n",
    "                + ',' + \"number observations over cross validation\")\n",
    "        \n",
    "    return output_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8038c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_stats(cv_results, best_params, folds):\n",
    "    \"\"\"Method to get model statistics from training run, like mean, standard deviation and number of observations.\n",
    "    This helps calculating statistical significance in the end.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cv_results : dict\n",
    "        dictionary with results from cross-validation, e.g. train/test mean/std. dev. \n",
    "    best_params: list:\n",
    "        best parameter configuration\n",
    "    folds : number of cross validation folds\n",
    "    \"\"\"\n",
    "    truth = [param == best_params for param in cv_results['params']]\n",
    "            \n",
    "    curr_model_stats = {}\n",
    "    curr_model_stats[\"train\"] = {}\n",
    "    curr_model_stats[\"train\"][\"mean\"] = cv_results['mean_train_score'][truth][0]\n",
    "    curr_model_stats[\"train\"][\"std_dev\"] = cv_results['std_train_score'][truth][0]\n",
    "    curr_model_stats[\"train\"][\"observations\"] = folds\n",
    "    curr_model_stats[\"test\"] = {}\n",
    "    curr_model_stats[\"test\"][\"mean\"] = cv_results['mean_test_score'][truth][0]\n",
    "    curr_model_stats[\"test\"][\"std_dev\"] = cv_results['std_test_score'][truth][0]\n",
    "    curr_model_stats[\"test\"][\"observations\"] = folds\n",
    "    \n",
    "    print(\"Mean: {}\".format(cv_results['mean_test_score'][truth]))\n",
    "    print(\"Standard deviation: {}\".format(cv_results['std_test_score'][truth]))\n",
    "    print(\"Number of observations: {}\".format(folds))\n",
    "    \n",
    "    return curr_model_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25fc3768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_model_stats(hist):\n",
    "#     \"\"\"\n",
    "#     Plot training curve over time (for neural networks)\n",
    "#     \"\"\"\n",
    "#     plt.plot(hist.history['acc'])\n",
    "#     plt.plot(hist.history['val_acc'])\n",
    "#     plt.title('model accuracy')\n",
    "#     plt.ylabel('accuracy')\n",
    "#     plt.xlabel('epoch')\n",
    "#     plt.legend(['train', 'test'], loc='upper left')\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Summarize history for loss\n",
    "#     plt.plot(hist.history['loss'])\n",
    "#     plt.plot(hist.history['val_loss'])\n",
    "#     plt.title('model loss')\n",
    "#     plt.ylabel('loss')\n",
    "#     plt.xlabel('epoch')\n",
    "#     plt.legend(['train', 'test'], loc='upper left')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b121211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_model(benchmark_models, parameter_candidates, train, test, model_name, \n",
    "                      output_file_name, folds = 10, prefix = \"00000000\"):\n",
    "    \"\"\"Perform grid search with model parameters and save results to CSV file and produced models\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    benchmark_models : dict\n",
    "        key value pair of model_name:model.\n",
    "    parameter_candidates : dict\n",
    "        key value pair of model_name:model parameters.\n",
    "    train : list\n",
    "        list with values input data,target that contains the train data\n",
    "    test : list\n",
    "        list with values input data,target that contains the test data\n",
    "    model_name : string\n",
    "        name of dataset model that was used to create intermediate dataset (e.g., \"CIFAR-100\")\n",
    "    output_file_name : string\n",
    "        name of the output file (that has been previously created)\n",
    "    folds : int\n",
    "        number of folds for crossvalidation\n",
    "    prefix : string\n",
    "        prefix for output file \n",
    "    \"\"\"\n",
    "    model_stats = {}\n",
    "    \n",
    "    # The following process is done for every model in the list of benchmark_models\n",
    "    for model_key in benchmark_models.keys():\n",
    "        model = benchmark_models[model_key]\n",
    "        print ((\" Model: \" + str(model_key)+ \" \").center(30, '#'))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # The model could be either from sklearn or from Keras\n",
    "        # If the following try statement fails, we expect the model to be a Keras model, \n",
    "        # because GridSearchCV does not work with Keras models\n",
    "        try:\n",
    "            # Initialize grid search for model with specified parameters from parameter_candidates\n",
    "            #print(model)\n",
    "            #print(parameter_candidates[model_key])\n",
    "            #print(folds)\n",
    "            clf = GridSearchCV(estimator=model, \n",
    "                               param_grid=parameter_candidates[model_key], \n",
    "                               n_jobs=-1, \n",
    "                               cv=folds, \n",
    "                               return_train_score=True)\n",
    "            \n",
    "            # Perform grid search by fitting each parameter configuration to the training data\n",
    "            #clf.fit(*train)\n",
    "            clf.fit(train[0], train[1])\n",
    "            model = clf.best_estimator_\n",
    "            print (\"Best parameters: {}\".format(clf.best_params_))\n",
    "            \n",
    "            path = \"/home/ruifi/thesis/outputs/benchmark_results/\" + prefix + \"_\" + dataset_name + \"/\" + model_name + \"/\"\n",
    "            print(path)\n",
    "            \n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "                \n",
    "            # Save model\n",
    "            filename = path + model_key + \".sav\"\n",
    "\n",
    "            try:\n",
    "                pickle.dump(clf, open(filename, 'wb'))\n",
    "            except:\n",
    "                print(filename + \" could not be created.\")\n",
    "                \n",
    "            # Get the model statistics from method defined before\n",
    "            curr_model_stats = get_model_stats(clf.cv_results_, clf.best_params_, folds)\n",
    "        \n",
    "        except:\n",
    "            model = benchmark_models[model_key][model_name]\n",
    "            \n",
    "            # Define k-fold with number of folds from function parameters\n",
    "            kf = KFold(n_splits=folds)\n",
    "            \n",
    "            # Keras expects target to be one hot encoded\n",
    "            target_cat = np.array(to_categorical(train[1]))\n",
    "            \n",
    "            # Initialize model stats to be able to perform statistical tests\n",
    "            cv_results = {}\n",
    "            cv_results[\"train\"] = []\n",
    "            cv_results[\"test\"] = []\n",
    "            curr_model_stats = {\n",
    "                    \"train\": {\n",
    "                        \"mean\":0,\n",
    "                        \"std_dev\":0,\n",
    "                        \"observations\":0\n",
    "                    },\n",
    "                    \"test\": {\n",
    "                        \"mean\":0,\n",
    "                        \"std_dev\":0,\n",
    "                        \"observations\":0\n",
    "                    }\n",
    "                }\n",
    "            \n",
    "            # Save weights to always initialize model with identical weights\n",
    "            Wsave = model.get_weights()\n",
    "            \n",
    "            for epoch in epochs:\n",
    "                # Perform n-fold split\n",
    "                for kfold_ind, indices in enumerate(kf.split(train[0])):\n",
    "                    model.set_weights(Wsave)\n",
    "                    train_index, test_index = indices\n",
    "                    X_train, X_test = train[0][train_index], train[0][test_index]\n",
    "                    y_train, y_test = target_cat[train_index], target_cat[test_index]\n",
    "                    \n",
    "                    # Train model\n",
    "                    history = model.fit(X_train, y_train, epochs = epoch, validation_data = (X_test, y_test))\n",
    "                    \n",
    "                    path = \"/home/ruifi/thesis/outputs/benchmark_results\" + prefix + \"_\" + dataset_name + \"/\" + model_name + \"/\"\n",
    "                    \n",
    "                    if not os.path.exists(path):\n",
    "                        os.makedirs(path)\n",
    "                    \n",
    "                    # Save weights of model for future reuse (if necessary)\n",
    "                    model.save_weights(path + str(kfold_ind) + \"_\" + model_key + \".h5\")\n",
    "                    print(model_key)\n",
    "                    \n",
    "                    # Plot model learning curve\n",
    "                    plot_model_stats(history)\n",
    "                    \n",
    "                    # Evaluate model on training data\n",
    "                    train_res = model.evaluate(X_train, y=y_train)[1]\n",
    "                    \n",
    "                    # Evaluate model on unseen test data\n",
    "                    test_res = model.evaluate(X_test, y=y_test)[1]\n",
    "                    \n",
    "                    # Save results\n",
    "                    cv_results[\"train\"].append(train_res)\n",
    "                    cv_results[\"test\"].append(test_res)\n",
    "                    \n",
    "                # Overwrite statistics if they have been better\n",
    "                if curr_model_stats[\"test\"][\"mean\"] < statistics.mean(cv_results[\"test\"]):\n",
    "                    curr_model_stats = {\n",
    "                        \"train\": {\n",
    "                            \"mean\":statistics.mean(cv_results[\"train\"]),\n",
    "                            \"std_dev\":statistics.stdev(cv_results[\"train\"]),\n",
    "                            \"observations\":len(cv_results[\"train\"])\n",
    "                        },\n",
    "                        \"test\": {\n",
    "                            \"mean\":statistics.mean(cv_results[\"test\"]),\n",
    "                            \"std_dev\":statistics.stdev(cv_results[\"test\"]),\n",
    "                            \"observations\":len(cv_results[\"test\"])\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "        model_stats[model_key] = curr_model_stats           \n",
    "\n",
    "        # Evaluate final model with evaluate function from above (various metrics)\n",
    "        train_eval = evaluate(model, train)\n",
    "        test_eval = evaluate(model, test)\n",
    "        \n",
    "        # Print evaluation metrics\n",
    "        print_measures(train_eval, \"Train\")\n",
    "        print_measures(test_eval, \"Test\")\n",
    "        print(\"Total Time: {}\".format(time.time() - start_time))\n",
    "        \n",
    "        # Save evaluation metrics in file\n",
    "        write_to_file(train_eval, test_eval, curr_model_stats, model_key, model_name, output_file_name)\n",
    "\n",
    "    return benchmark_models, model_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931d487b",
   "metadata": {},
   "source": [
    "## 1. Loading the datasets\n",
    "\n",
    "The datasets are pre split between train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9028f258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ruifi/thesis'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check current path\n",
    "import os\n",
    "\n",
    "first = os.getcwd()\n",
    "first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "072fa01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the intermediate datasets\n",
    "path = first + '/outputs/data/cifar10_filtered/intermediate' # For CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5c4d032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the subfolder in path to use\n",
    "predefined_folder_name = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3aa92fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If no predefined folder name is set, the latest data is used (folder with latest timestamp)\n",
    "if predefined_folder_name == \"\":\n",
    "    folder_list = np.sort(glob.glob(path + \"/*\"))\n",
    "    curr_path = folder_list[-1] + \"/\"\n",
    "    \n",
    "else:\n",
    "    curr_path = path + predefined_folder_name + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3ffc928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ruifi/thesis/outputs/data/cifar10_filtered/intermediate/20220224_1845/'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The current path will then consist of the path and the latest timestamp folder\n",
    "curr_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cbdfc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the dataset\n",
    "dataset_name = curr_path.split(\"/\")[-4].split(\"_\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be920c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cifar10'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ffcde1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model names are all subfolders that are in the folder curr_path\n",
    "model_names = [fn.split(\"/\")[-1] for fn in glob.glob(curr_path + \"*\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b404effa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CNN_Input_1', 'CNN_Input_2']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5906b6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .npy files into Python\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for fn in model_names:\n",
    "    train_data_parts = np.load(glob.glob(curr_path + str(fn) + \"/train*\")[0], allow_pickle=True)\n",
    "    for tt in glob.glob(curr_path + str(fn) + \"/train*\")[1:]:\n",
    "        train_data_parts = np.vstack((train_data_parts,np.load(tt)))\n",
    "    train_data.append(copy.copy(train_data_parts))\n",
    "    \n",
    "    test_data_parts = np.load(glob.glob(curr_path + str(fn) + \"/test*\")[0], allow_pickle=True)\n",
    "    for tt in glob.glob(curr_path + str(fn) + \"/test*\")[1:]:\n",
    "        test_data_parts = np.vstack((test_data_parts, np.load(tt)))\n",
    "    test_data.append(copy.copy(test_data_parts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "155af90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_Input_1 dimension on input data: (10,)\n",
      "CNN_Input_2 dimension on input data: (10,)\n"
     ]
    }
   ],
   "source": [
    "# We can check the dataset dimensions\n",
    "for ind, name in enumerate(model_names):\n",
    "    print(name + \" dimension on input data: \" + str(train_data[ind][0][0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "153dfc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over train and test data to split the input data from the target for each\n",
    "train_data = iterate_over_datasets(train_data, split_input_target)\n",
    "test_data = iterate_over_datasets(test_data, split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "851da771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindex datasets to make their classes continuous?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072bde08",
   "metadata": {},
   "source": [
    "## 2. Classification Model Benchmark\n",
    "\n",
    "### M4GP hyperparameters\n",
    "\n",
    "Check all ellyn parameters here: <br>\n",
    "https://github.com/cavalab/ellyn/blob/03ebdf07f0bfdca30da1a9a0a0da2989e9b1153c/src/ellyn.py#L42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e342068",
   "metadata": {},
   "outputs": [],
   "source": [
    "m4gp_candidates= [\n",
    "    {'g': [100, 200], \n",
    "     'popsize': [50, 100], \n",
    "     'selection': ['tournament', 'lexicase'] #,\n",
    "     #'elitism': [True, False],\n",
    "     #'tourn_size': [2, 10, 20],\n",
    "     #'rt_cross': [0.5, 0.8],\n",
    "     #'rt_mut': [0.2, 0.5]\n",
    "     }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6b1f6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_candidates = {}\n",
    "\n",
    "parameter_candidates['m4gp'] = m4gp_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e29ab0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifiers in the benchmark\n",
    "\n",
    "# Parameters needed for M4GP on ellyn\n",
    "m4gp = ellyn(#verbosity=1, \n",
    "             class_m4gp=True, \n",
    "             op_list=['v','+','-','*','/','sin','cos','exp','log','sqrt'],\n",
    "                      #,'=','!','<','<=','>','>=','if-then','if-then-else','&','|'],\n",
    "             fit_type='F1')\n",
    "\n",
    "# Experiments:\n",
    "# op_list=['n','v','+','-','*','/','sin','cos','exp','log','sqrt']\n",
    "# random_state=42\n",
    "\n",
    "benchmark_models = {'m4gp': m4gp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65af3654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'m4gp': ellyn(class_m4gp=True, fit_type='F1',\n",
       "       op_list=['v', '+', '-', '*', '/', 'sin', 'cos', 'exp', 'log', 'sqrt'])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the benchmark model\n",
    "benchmark_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e6c459",
   "metadata": {},
   "source": [
    "Run benchmarking function (grid search model) for each dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cfdc793e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________\n",
      "############################################################\n",
      "##################### Dataset Index: 0 #####################\n",
      "############################################################\n",
      "######## Model: m4gp #########\n",
      "Best parameters: {'g': 200, 'popsize': 100, 'selection': 'lexicase'}\n",
      "/home/ruifi/thesis/outputs/benchmark_results/20220406_1804_cifar10/CNN_Input_1/\n",
      "Mean: [0.33528]\n",
      "Standard deviation: [0.10681443]\n",
      "Number of observations: 10\n",
      "y_proba, is it a problem?: [6 4 4 ... 4 1 1]\n",
      "y_proba, is it a problem?: [1 3 1 ... 5 1 4]\n",
      "accuracy in Train: 0.53\n",
      "confusion matrix\n",
      "[[ 218    1    0   76 4032    0    0  306  200  167]\n",
      " [   0 5000    0    0    0    0    0    0    0    0]\n",
      " [   0    0 4995    1    4    0    0    0    0    0]\n",
      " [ 474   18    0 1046 1586    0    2  436  551  887]\n",
      " [ 161    0    0   20 4416    0    0  197  109   97]\n",
      " [   0    0    0    0    0 4986   14    0    0    0]\n",
      " [   0    6    0    0    0    0 4994    0    0    0]\n",
      " [  73    0    0   31 4697    0    0   93   54   52]\n",
      " [ 101    2    0  101 4439    0    0  138  105  114]\n",
      " [ 263    3    0  355 3286    0    0  360  323  410]]\n",
      "precision in Train: 0.54\n",
      "recall in Train: 0.53\n",
      "f1-score in Train: 0.49\n",
      "cohen's kappa in Train: 0.47\n",
      "accuracy in Test: 0.43\n",
      "confusion matrix\n",
      "[[ 37  26  22 110 653   1   8  41  34  68]\n",
      " [  1 948   0  34   2   2   7   2   0   4]\n",
      " [  8  36 677  71 114  24  41   7   5  17]\n",
      " [ 43  82  44 308 162  82 115  34  43  87]\n",
      " [ 32  18  18 131 607   5  23  51  51  64]\n",
      " [  6  38  25 111  34 663  98   3   6  16]\n",
      " [  0  63   8  51   5   4 867   1   0   1]\n",
      " [ 26  19  13 110 697  20  27  25  30  33]\n",
      " [ 31  42   0  76 756   3   2  22  30  38]\n",
      " [ 50  73   1 138 555   0   4  40  45  94]]\n",
      "precision in Test: 0.41\n",
      "recall in Test: 0.43\n",
      "f1-score in Test: 0.39\n",
      "cohen's kappa in Test: 0.36\n",
      "Total Time: 185.50459837913513\n",
      "/home/ruifi/thesis/outputs/benchmark_results/20220406_1804_cifar10/20220406_1804_0_train.csv\n",
      "____________________________________________________________\n",
      "############################################################\n",
      "##################### Dataset Index: 1 #####################\n",
      "############################################################\n",
      "######## Model: m4gp #########\n",
      "Best parameters: {'g': 200, 'popsize': 50, 'selection': 'lexicase'}\n",
      "/home/ruifi/thesis/outputs/benchmark_results/20220406_1804_cifar10/CNN_Input_2/\n",
      "Mean: [0.28448]\n",
      "Standard deviation: [0.07691591]\n",
      "Number of observations: 10\n",
      "y_proba, is it a problem?: [1 9 9 ... 9 6 3]\n",
      "y_proba, is it a problem?: [3 0 0 ... 5 9 6]\n",
      "accuracy in Train: 0.65\n",
      "confusion matrix\n",
      "[[4977    0    0    1    0   22    0    0    0    0]\n",
      " [   3  604    5  855    0   38 3492    3    0    0]\n",
      " [  12  484  812  766    5   80 2525  262    0   54]\n",
      " [   0  700  445 2070    0  146 1439  168    0   32]\n",
      " [   1    0   12    1 4840    1    0    0    0  145]\n",
      " [   0    0   26   51    2 4909    2    1    0    9]\n",
      " [   0  179  255  176    0    3 4191  192    0    4]\n",
      " [   0  335  510  390    1    5 3471  276    0   12]\n",
      " [ 147    0    0    0    0    1    0    0 4852    0]\n",
      " [   5    0  162   19    0   33    3    3    1 4774]]\n",
      "precision in Train: 0.65\n",
      "recall in Train: 0.65\n",
      "f1-score in Train: 0.62\n",
      "cohen's kappa in Train: 0.61\n",
      "accuracy in Test: 0.50\n",
      "confusion matrix\n",
      "[[836   5  12  33   4  78   7   1  13  11]\n",
      " [ 18  81  16 199   0  84 563   3   6  30]\n",
      " [ 44  68 133 183  45 113 341  26   2  45]\n",
      " [ 13  44  98 313  28 252 183  19   1  49]\n",
      " [ 11   4  94  32 650  31   9   6   4 159]\n",
      " [ 12   9  44 115  12 772   9   2   0  25]\n",
      " [  5  21  85  78  10  36 708  37   1  19]\n",
      " [  8  44 130 128  17  66 535  35   0  37]\n",
      " [158   3   8  19   0  41   2   0 761   8]\n",
      " [ 33   8  76  49   1  75   7   3   7 741]]\n",
      "precision in Test: 0.50\n",
      "recall in Test: 0.50\n",
      "f1-score in Test: 0.47\n",
      "cohen's kappa in Test: 0.45\n",
      "Total Time: 147.5970437526703\n",
      "/home/ruifi/thesis/outputs/benchmark_results/20220406_1804_cifar10/20220406_1804_1_train.csv\n"
     ]
    }
   ],
   "source": [
    "final_models = []\n",
    "stats = {}\n",
    "\n",
    "# Save current time to add as a prefix to file output names\n",
    "now = datetime.datetime.now()\n",
    "prefix = now.strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "# Perform benchmark model comparison for each dataset\n",
    "for ind, _ in enumerate(train_data):\n",
    "    print ((\"\").center(60, '_'))\n",
    "    print ((\"\").center(60, '#'))\n",
    "    print ((\" Dataset Index: \" + str(ind)+ \" \").center(60, '#'))\n",
    "    print ((\"\").center(60, '#'))\n",
    "    \n",
    "    # Create CSV files to write output to and initalize those with table headers\n",
    "    file_name = create_data_files(ind, prefix)\n",
    "    \n",
    "    # Perform grid search with method above\n",
    "    m,stat = grid_search_model(benchmark_models, \n",
    "                               parameter_candidates, \n",
    "                               train_data[ind], \n",
    "                               test_data[ind], \n",
    "                               model_names[ind], \n",
    "                               file_name, \n",
    "                               10,\n",
    "                               prefix)\n",
    "        \n",
    "    final_models.append(m)\n",
    "    \n",
    "    # Save model statistics\n",
    "    stats[model_names[ind]] = stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e734a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96c6093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prob 1) I think ellyn doesnt have model.predict_proba. This is a big issue when using 'get_top_n_accuracy'!\n",
    "# Prob 2) Very low accuracy? The models seem very strange, sometimes they are only decent for one class, \n",
    "# and very bad for the others.\n",
    "\n",
    "\n",
    "\n",
    "# I will print y_proba just to check AND I'll comment the top-n-accuracy for now.\n",
    "# I can check the previous outputs in my github!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66009830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed1c779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405dad6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f00855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418df724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694a21ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24ad32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_signficance_matrix(stats, base=\"test\", sig_level=0.05, one_sided = False):\n",
    "    \"\"\"Return significance matrix of multiple benchmark comparisons\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    stats : dict\n",
    "        key value pair of model_name : statistics\n",
    "        with the structure model_name -> classification_algorithm -> train/test -> mean/std_dev/observations\n",
    "    base : \"string\"\n",
    "        dataset to perform comparison on, e.g. \"train\", \"test\" (default: test)\n",
    "    sig_level : float\n",
    "        significance level to test for\n",
    "    one_sided : boolean\n",
    "        boolean to determine if test should be one sided or not\n",
    "\n",
    "    returns a dictionary with each being (dataset:significance_matrix as pandas DF), e.g., sig_matrix_dict[\"InceptionV3\"]\n",
    "    \"\"\"\n",
    "    if one_sided:\n",
    "        sig_level /= 2\n",
    "        \n",
    "    sig_matrix_dict = {}\n",
    "    \n",
    "    for dataset in stats.keys():\n",
    "        sig_matrix_dict[dataset] = []\n",
    "        model_entries = stats[dataset].keys()\n",
    "        \n",
    "        for model_key in stats[dataset].keys():\n",
    "            dim_B = []\n",
    "            \n",
    "            for compare_model_key in stats[dataset].keys():\n",
    "                t_stats = ttest_ind_from_stats(stats[dataset][model_key][base][\"mean\"], \n",
    "                                               stats[dataset][model_key][base][\"std_dev\"], \n",
    "                                               stats[dataset][model_key][base][\"observations\"], \n",
    "                                               stats[dataset][compare_model_key][base][\"mean\"], \n",
    "                                               stats[dataset][compare_model_key][base][\"std_dev\"], \n",
    "                                               stats[dataset][compare_model_key][base][\"observations\"])\n",
    "                \n",
    "                if t_stats[1] < sig_level and t_stats[0] < 0.0:\n",
    "                    dim_B.append(True)\n",
    "                else:\n",
    "                    dim_B.append(False)\n",
    "                    \n",
    "            sig_matrix_dict[dataset].append(dim_B)\n",
    "            \n",
    "        sig_matrix_dict[dataset] = pd.DataFrame(sig_matrix_dict[dataset],index=model_entries,columns=model_entries)\n",
    "        \n",
    "    return sig_matrix_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdbcab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_signficance_matrix(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37f94cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
