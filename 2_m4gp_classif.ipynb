{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e12c129",
   "metadata": {},
   "source": [
    "# Stage 2: Image Feature Analysis\n",
    "\n",
    "**Rui Filipe Martins Monteiro (R20170796) | MSc in Data Science and Advanced Analytics**\n",
    "\n",
    "<br>\n",
    "\n",
    "This notebook loads the intermediate datasets and performs classification with M4GP. Significance tests are performed on the output to determine statistical significance.\n",
    "\n",
    "[ellyn](https://cavalab.org/ellyn/), a genetic programming system for regression, is one of the most important Python libraries. It is needed to apply M4GP.\n",
    "\n",
    "<br>\n",
    "\n",
    "This code is heavily inspired by: Jonathan Janke (https://github.com/novajon/classy-conv-features)\n",
    "\n",
    "Code changed and improved by: Rui Monteiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f8eb222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "from ellyn import ellyn\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    confusion_matrix, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    roc_auc_score, \n",
    "    cohen_kappa_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, KFold\n",
    "\n",
    "# import keras\n",
    "# from keras.utils import to_categorical\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "# from scipy import stats\n",
    "# import time\n",
    "# import datetime\n",
    "# import os\n",
    "# from scipy.stats import ttest_ind_from_stats\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pickle\n",
    "# import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9028f258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ruifi/thesis'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check current path\n",
    "import os\n",
    "\n",
    "first = os.getcwd()\n",
    "first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "072fa01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the intermediate datasets\n",
    "path = first + '/outputs/data/cifar10_filtered/intermediate' # For CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5c4d032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the subfolder in path to use\n",
    "predefined_folder_name = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3aa92fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If no predefined folder name is set, the latest data is used (folder with latest timestamp)\n",
    "if predefined_folder_name == \"\":\n",
    "    folder_list = np.sort(glob.glob(path + \"/*\"))\n",
    "    curr_path = folder_list[-1] + \"/\"\n",
    "    \n",
    "else:\n",
    "    curr_path = path + predefined_folder_name + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3ffc928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ruifi/thesis/outputs/data/cifar10_filtered/intermediate/20220224_1845/'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The current path will then consist of the path and the latest timestamp folder\n",
    "curr_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cbdfc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the dataset\n",
    "dataset_name = curr_path.split(\"/\")[-4].split(\"_\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be920c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cifar10'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ffcde1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model names are all subfolders that are in the folder curr_path\n",
    "model_names = [fn.split(\"/\")[-1] for fn in glob.glob(curr_path + \"*\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b404effa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CNN_Input_1', 'CNN_Input_2']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931d487b",
   "metadata": {},
   "source": [
    "## 1. Loading the datasets\n",
    "\n",
    "The datasets are pre split between train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5906b6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for fn in model_names:\n",
    "    train_data_parts = np.load(glob.glob(curr_path + str(fn) + \"/train*\")[0], allow_pickle=True)\n",
    "    for tt in glob.glob(curr_path + str(fn) + \"/train*\")[1:]:\n",
    "        train_data_parts = np.vstack((train_data_parts,np.load(tt)))\n",
    "    train_data.append(copy.copy(train_data_parts))\n",
    "    \n",
    "    test_data_parts = np.load(glob.glob(curr_path + str(fn) + \"/test*\")[0], allow_pickle=True)\n",
    "    for tt in glob.glob(curr_path + str(fn) + \"/test*\")[1:]:\n",
    "        test_data_parts = np.vstack((test_data_parts, np.load(tt)))\n",
    "    test_data.append(copy.copy(test_data_parts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "155af90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_Input_1 dimension on input data: (10,)\n",
      "CNN_Input_2 dimension on input data: (10,)\n"
     ]
    }
   ],
   "source": [
    "# We can check the dataset dimensions\n",
    "for ind, name in enumerate(model_names):\n",
    "    print(name + \" dimension on input data: \" + str(train_data[ind][0][0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0e56a084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_over_datasets(datasets, function):\n",
    "    \"\"\"Iterate over datasets and apply function on each one\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datasets : list\n",
    "        list of datasets that function should be applied on\n",
    "    function : function\n",
    "        function that should be applied on each dataset\n",
    "    \"\"\"\n",
    "    ret_data = []\n",
    "    \n",
    "    for data in datasets:\n",
    "        n_data = function(data)\n",
    "        ret_data.append(n_data)\n",
    "        \n",
    "    return ret_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f56678db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target (data):\n",
    "    \"\"\"Save the target data separately from the input data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : list\n",
    "        list containing input and output data together\n",
    "    \"\"\"\n",
    "    target = []\n",
    "    inp = []\n",
    "    \n",
    "    for d in data:\n",
    "        target.append(d[1][0])\n",
    "        inp.append(d[0])\n",
    "        \n",
    "    return [np.array(inp), target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "153dfc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over train and test data to split the input data from the target for each\n",
    "train_data = iterate_over_datasets(train_data, split_input_target)\n",
    "test_data = iterate_over_datasets(test_data, split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a69716a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex_classes(data):\n",
    "    \"\"\"Reindexing classes to be continuous and start at 0\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : list\n",
    "        dataset to reindex\n",
    "    \"\"\"\n",
    "    for ind, ds in enumerate(data):\n",
    "        ds[1] = [x - min (ds[1]) for x in ds[1]]\n",
    "        data[ind] = ds\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "851da771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindex datasets to make their classes continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "95c54ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_accuracy(input_target, input_data, n=5):\n",
    "    \"\"\"Get top n accuracy from input data (predictions)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_target : list\n",
    "        array of correct targets\n",
    "    input_data : list\n",
    "        array of predictions\n",
    "    n : int\n",
    "        n value for top-n-accuracy\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    inp_data = input_data.copy()\n",
    "    \n",
    "    for ind, pred in enumerate(inp_data):\n",
    "        max_classes = []\n",
    "        \n",
    "        for i in range (n):\n",
    "            max_classes.append(np.argmax(pred))\n",
    "            pred[np.argmax(pred)]=-1\n",
    "            \n",
    "        if input_target[ind] in max_classes: \n",
    "            count += 1\n",
    "            \n",
    "    return count/len(input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "58433483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    \"\"\"Method to evaluate a model based on several metrics:\n",
    "        - top-1-accuracy (\"accuracy\")\n",
    "        - top-2-accuracy\n",
    "        - top-5-accuracy\n",
    "        - top-10-accuracy\n",
    "        - top-20-accuracy\n",
    "        - confusion matrix\n",
    "        - precision\n",
    "        - recall\n",
    "        - f1 score\n",
    "        - cohens kappa\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : keras/sklearn model (needs predict function)\n",
    "        dataset to reindex\n",
    "\n",
    "    data : list\n",
    "        dataset to evaluate\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(data[0])\n",
    "    \n",
    "    try: y_proba = model.predict_proba(data[0])\n",
    "    except: y_proba = model.predict(data[0])\n",
    "    \n",
    "    try: predictions = [round(value) for value in y_pred]\n",
    "    except: predictions = [np.argmax(value) for value in y_pred]\n",
    "    \n",
    "    # Evaluate predictions\n",
    "    d = {}\n",
    "    d[\"accuracy\"] = accuracy_score(data[1], predictions)\n",
    "    d[\"top-2-accuracy\"] = get_top_n_accuracy(data[1], y_proba, n=2)\n",
    "    d[\"top-5-accuracy\"] = get_top_n_accuracy(data[1], y_proba, n=5)\n",
    "    d[\"top-10-accuracy\"] = get_top_n_accuracy(data[1], y_proba, n=10)\n",
    "    d[\"top-20-accuracy\"] = get_top_n_accuracy(data[1], y_proba, n=20)\n",
    "    d[\"confusion matrix\"] = confusion_matrix(data[1], predictions)\n",
    "    d[\"precision\"] = precision_score(data[1], predictions, average='macro')\n",
    "    d[\"recall\"] = recall_score(data[1], predictions, average='macro')\n",
    "    d[\"f1-score\"] = f1_score(data[1], predictions, average='macro')\n",
    "    # d[\"roc-auc\"] = roc_auc_score(data[1], predictions, )\n",
    "    d[\"cohen's kappa\"] = cohen_kappa_score(data[1], predictions)\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b5ba3bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_measures(measure, data_type=\"data\", measure_name=\"Accuracy\"):\n",
    "    \"\"\"Get string representation of measures\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    measure : float\n",
    "        value to print\n",
    "    data_type : string\n",
    "        usually test or training data\n",
    "    measure_name : string\n",
    "        measure that is applied, e.g. 'Accuracy'\n",
    "    \"\"\"\n",
    "    return \"%s in %s: %.2f\" % (measure_name, data_type, measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b0e9abfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_measures(evaluation, t):\n",
    "    \"\"\"Print measures on screen\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    evaluation : dict\n",
    "        key value pairs with accuracy measure : value\n",
    "    t : string\n",
    "        Data type, e.g. \"Test\" or \"Train\"\n",
    "    \"\"\"\n",
    "    for key in evaluation.keys():\n",
    "        if key != \"confusion matrix\":\n",
    "            print(get_measures(evaluation[key], t, key))\n",
    "            \n",
    "        else:\n",
    "            print(key)\n",
    "            print(evaluation[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "72168829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(train_eval, test_eval, model_stats, model_key, model_name, output_file_name):\n",
    "    \"\"\"Save the results to a CSV file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_eval : dict\n",
    "        key value pairs of \"evaluation metric name : evaluation metric value\" on train data\n",
    "    test_eval : dict\n",
    "        key value pairs of \"evaluation metric name : evaluation metric value\" on test data\n",
    "    model_stats : dict\n",
    "        key value pair of \"train\"/\"test\" : model statistics (e.g., mean, std. dev, ...)\n",
    "    model_key : string\n",
    "        name of the model that was applied\n",
    "    model_name : string\n",
    "        name of the model that was used to create the intermediate dataset, e.g., VGG-16\n",
    "    output_file_name : string\n",
    "        name of the file to write to\n",
    "    \"\"\"\n",
    "    file_stamp = curr_path.split(\"/\")[-2]\n",
    "    train_size = str(len(train_data[0][0]))\n",
    "    test_size = str(len(test_data[0][0]))\n",
    "    dataset_dim = str(np.max(train_data[0][1] + test_data[0][1]) + 1)\n",
    "    \n",
    "    with open(output_file_name + '_train.csv', 'a') as f:\n",
    "        print (output_file_name + '_train.csv')\n",
    "        f.write(\"\\n\" + model_name + ',' + file_stamp + ',' + train_size + ',' + dataset_dim + ',' + model_key + \",\")\n",
    "        f.write(','.join([str(train_eval[x]) for x in train_eval.keys() if x!=\"confusion matrix\"]) + \",\")\n",
    "        f.write(','.join([str(x) for x in model_stats[\"train\"].values()]))\n",
    "    \n",
    "    with open(output_file_name + '_test.csv', 'a') as f:\n",
    "        f.write(\"\\n\" + model_name + ',' + file_stamp + ',' + test_size + ',' + dataset_dim + ',' + model_key + \",\")\n",
    "        f.write(','.join([str(test_eval[x]) for x in test_eval.keys() if x!=\"confusion matrix\"]) + \",\")    \n",
    "        f.write(','.join([str(x) for x in model_stats[\"test\"].values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f9750e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_files(ind,prefix):\n",
    "    \"\"\"Create initial files to write to\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ind : int\n",
    "        index of dataset\n",
    "    prefix : string\n",
    "        prefix to give to file name\n",
    "    \"\"\"\n",
    "    path = \"/home/ruifi/thesis/outputs/benchmark_results\" + dataset_name + \"/\"\n",
    "    output_file_name = path + prefix + \"_\" + str(ind)\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    with open(output_file_name + '_train.csv', 'w') as f:\n",
    "        f.write('Dataset,Dataset Stamp,Dataset Size,# Dataset Classes,Model Architecture' \n",
    "                + ',' + \"accuracy\" + ',' + \"top-2-accuracy\" + ',' + \"top-5-accuracy\" + ',' \n",
    "                + \"top-10-accuracy\" + ',' + \"top-20-accuracy\" + ',' + \"precision\" + ',' \n",
    "                + \"recall\" + ',' + \"f1-score\" + ',' + \"cohen's kappa\" + ',' \n",
    "                + \"Mean Accuracy over cross validation\" + ',' + \"mean standard deviation over cross validation\" \n",
    "                + ',' + \"number observations over cross validation\")\n",
    "        \n",
    "    with open(output_file_name + '_test.csv', 'w') as f:\n",
    "        f.write('Dataset,Dataset Stamp,Dataset Size,# Dataset Classes,Model Architecture' \n",
    "                + ',' + \"accuracy\" + ',' + \"top-2-accuracy\" + ',' + \"top-5-accuracy\" + ',' \n",
    "                + \"top-10-accuracy\" + ',' + \"top-20-accuracy\" + ',' + \"precision\" \n",
    "                + ',' + \"recall\" + ',' + \"f1-score\" + ',' + \"cohen's kappa\" + ',' \n",
    "                + \"Mean Accuracy over cross validation\" + ',' + \"mean standard deviation over cross validation\" \n",
    "                + ',' + \"number observations over cross validation\")\n",
    "        \n",
    "    return output_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072bde08",
   "metadata": {},
   "source": [
    "## 2. Classification Model Benchmark\n",
    "\n",
    "### M4GP hyperparameters\n",
    "\n",
    "Check all parameters here: <br>\n",
    "https://github.com/cavalab/ellyn/blob/03ebdf07f0bfdca30da1a9a0a0da2989e9b1153c/src/ellyn.py#L42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5a16ed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = ellyn(\n",
    "    #g=200, \n",
    "    #popsize=50,\n",
    "    verbosity=1,\n",
    "    class_m4gp=True,\n",
    "    op_list=['n','v','+','-','*','/', 'sin', 'cos', 'exp', 'log', 'sqrt'],\n",
    "    #selection='lexicase',\n",
    "    #fit_type='F1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e342068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# START CHECKING ON \"Create other classification models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b1f6c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29ab0b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65af3654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbffff5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96c6093",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
